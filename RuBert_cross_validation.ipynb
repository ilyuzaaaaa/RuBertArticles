{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Копия блокнота \"RuBertArticles.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilyuzaaaaa/RuBertArticles/blob/main/RuBert_cross_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGz8z13vF84k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca8000d-8952-4d8b-8cd8-916df72f1878"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIC6AQ5hKgtV"
      },
      "source": [
        "!cp \"gdrive/My Drive/Transformer/modeling.py\" .\n",
        "!cp \"gdrive/My Drive/Transformer/optimization.py\" .\n",
        "!cp \"gdrive/My Drive/Transformer/tokenization.py\" ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2_VPDaUJLTy"
      },
      "source": [
        "import tokenization \n",
        "import torch\n",
        "import sys \n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from modeling import BertConfig, BertForSequenceClassification\n",
        "from optimization import BERTAdam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob5ceiKvIxj4"
      },
      "source": [
        "df = pd.read_csv('./gdrive/My Drive/dataset_5.csv', encoding='utf8')\n",
        "df = df[['topics', 'title']]\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGHclaZXJErP"
      },
      "source": [
        "maps = pd.factorize(df.topics)[1]\n",
        "print(maps)\n",
        "df['topics'] = pd.factorize(df.topics)[0]\n",
        "print(df.head())\n",
        "df['title'] = df['title'].astype('str')\n",
        "for c in df:\n",
        "    if df[c].dtype == 'object':\n",
        "        print('Max длина предложения %s: %s\\n' %  (c, df[c].map(len).max())) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAlqGGkUj43W"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0854Ic2K2zm"
      },
      "source": [
        "sentences = df['title'].values\n",
        "labels = df['topics'].values # Никакой разницы что list, что values, просто удобство с размерностями для меня\n",
        "assert len(sentences) == len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGrem6pgJlUO"
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, label):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.label = label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euvah3iaJwgO"
      },
      "source": [
        "train_sentences, test_sentences, train_gt, test_gt = train_test_split(sentences, labels, shuffle=True,test_size=0.3, random_state=42)\n",
        "# shuffle - перемешивание данных, random_state - фиксируем генератор, чтобы разбивка датасета была случайна, но повторялась от запуска к запуску\n",
        "# train_gt, test_gt - ответы, которые соответсвуют предложениям (номера классов)\n",
        "assert len(set(train_gt)) == len(set(test_gt))\n",
        "num_classes = len(set(train_gt)) # сет нужен чтобы узнать количество классов (сколько уникальных номеров классов (у нас 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2sMk-Zmqzs8"
      },
      "source": [
        "kf = KFold(n_splits=3, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEq6R3ter6JV"
      },
      "source": [
        "def preprocessing(sentences, labels, tokenizer, max_len):\r\n",
        "    features = []\r\n",
        "    for i,sentence in enumerate(sentences):\r\n",
        "        \r\n",
        "        tokens_a = tokenizer.tokenize(sentence)\r\n",
        "        \r\n",
        "        if len(tokens_a) > max_len - 2:\r\n",
        "            tokens_a = tokens_a[0:(max_len - 2)]# берт принимает на вход последовательности длинны не более 512, поэтому мы проверяем, большо ли \r\n",
        "\r\n",
        "        tokens = []\r\n",
        "        tokens.append(\"[CLS]\")\r\n",
        "        for token in tokens_a:\r\n",
        "            tokens.append(token)\r\n",
        "        tokens.append(\"[SEP]\")\r\n",
        "\r\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "\r\n",
        "        input_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "        # Zero-pad up to the sequence length.\r\n",
        "        while len(input_ids) < max_len:\r\n",
        "            input_ids.append(0)\r\n",
        "            input_mask.append(0)\r\n",
        "            \r\n",
        "        assert len(input_ids) == max_len\r\n",
        "        assert len(input_mask) == max_len\r\n",
        "\r\n",
        "        features.append(\r\n",
        "                    InputFeatures(\r\n",
        "                            input_ids=input_ids,\r\n",
        "                            input_mask=input_mask,\r\n",
        "                            label=[labels[i]]))\r\n",
        "    \r\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQnbdAEXr7hl"
      },
      "source": [
        "class Dataload(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, features):\r\n",
        "        self.features = features\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.features)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "\r\n",
        "        \r\n",
        "        return torch.LongTensor(self.features[index].input_ids),\\\r\n",
        "               torch.LongTensor(self.features[index].input_mask),\\\r\n",
        "               torch.LongTensor(self.features[index].label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLSEDJLNsIFq"
      },
      "source": [
        "tokenizer = tokenization.FullTokenizer(vocab_file='./gdrive/My Drive/Transformer/vocab.txt', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PrGio7jq6MP"
      },
      "source": [
        "for fold, (train_index, test_index) in enumerate(kf.split(sentences, labels)):\r\n",
        "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index), fold)\r\n",
        "    train_sentences, test_sentences = sentences[train_index], sentences[test_index]\r\n",
        "    train_gt, test_gt = labels[train_index], labels[test_index]\r\n",
        "    \r\n",
        "    features = preprocessing(train_sentences, train_gt, tokenizer, 512)\r\n",
        "    dataset_train = Dataload(features)\r\n",
        "    train_dataloader = torch.utils.data.DataLoader(dataset_train,batch_size = 8, shuffle=True,\\\r\n",
        "                                                  num_workers=6, pin_memory=True)\r\n",
        "    features = preprocessing(test_sentences, test_gt, tokenizer, 512)\r\n",
        "    dataset_test = Dataload(features)\r\n",
        "    test_dataloader = torch.utils.data.DataLoader(dataset_test,batch_size = 1, shuffle=False,\\\r\n",
        "                                                  num_workers=6, pin_memory=True)\r\n",
        "    \r\n",
        "    print(len(train_dataloader))\r\n",
        "    print(len(test_dataloader))\r\n",
        "\r\n",
        "    device = 'cuda'\r\n",
        "    bert_config = BertConfig.from_json_file('./gdrive/My Drive/Transformer/bert_config.json')\r\n",
        "    model = BertForSequenceClassification(bert_config, num_classes)\r\n",
        "    model.bert.load_state_dict(torch.load('./gdrive/My Drive/Transformer/pytorch_model.bin'\\\r\n",
        "                                          , map_location='cpu'))\r\n",
        "    model.to(device)\r\n",
        "    model = torch.nn.DataParallel(model)\r\n",
        "\r\n",
        "    num_epoch = 5\r\n",
        "    no_decay = ['bias', 'gamma', 'beta']\r\n",
        "    optimizer_parameters = [\r\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\r\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\r\n",
        "        ]\r\n",
        "    num_train_steps = len(train_dataloader) * num_epoch\r\n",
        "    optimizer = BERTAdam(optimizer_parameters,\r\n",
        "                        lr=5e-5,\r\n",
        "                        warmup=0.1,\r\n",
        "                        t_total=num_train_steps)\r\n",
        "    \r\n",
        "    f = open('log'+str(fold)+'.txt', 'w')\r\n",
        "    f.close()\r\n",
        "\r\n",
        "\r\n",
        "    train_loss_set = []\r\n",
        "\r\n",
        "    batch_iterator = iter(train_dataloader)\r\n",
        "\r\n",
        "    total_step = 0\r\n",
        "    model.train()\r\n",
        "    train_loss = 0\r\n",
        "\r\n",
        "    while total_step<5*len(train_dataloader):\r\n",
        "\r\n",
        "        total_step += 1\r\n",
        "        try:\r\n",
        "            batch = next(batch_iterator)\r\n",
        "        except:\r\n",
        "            batch_iterator = iter(data_loader)\r\n",
        "            batch = next(batch_iterator)\r\n",
        "\r\n",
        "        batch = tuple(t.to(device) for t in batch)\r\n",
        "\r\n",
        "        b_input_ids, b_input_mask, b_labels = batch\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        loss, logits = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels.squeeze(1), token_type_ids=None)\r\n",
        "        \r\n",
        "        train_loss_set.append(loss.mean().item())\r\n",
        "\r\n",
        "        loss.mean().backward()\r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        train_loss += loss.mean().item()\r\n",
        "\r\n",
        "        clear_output(True)\r\n",
        "        plt.plot(train_loss_set)\r\n",
        "        plt.title(\"Training loss\")\r\n",
        "        plt.xlabel(\"Batch\")\r\n",
        "        plt.ylabel(\"Loss\")\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "        if total_step%1000 == 0:\r\n",
        "            print(\"Mean loss: {0:.5f}\".format(train_loss / len(train_dataloader)))\r\n",
        "            with open('log'+str(fold)+'.txt', 'a') as f:\r\n",
        "                f.write(\"Mean loss: {0:.5f}\\n\".format(train_loss / len(train_dataloader)))\r\n",
        "            torch.save(model.state_dict(), './gdrive/My Drive/weights'+str(fold)+'_'+str(total_step)+'.pth')\r\n",
        "            train_loss = 0\r\n",
        "            model.eval()\r\n",
        "            valid_preds, valid_labels = [], []\r\n",
        "\r\n",
        "            for batch in test_dataloader:   \r\n",
        "\r\n",
        "                batch = tuple(t.to(device) for t in batch)\r\n",
        "\r\n",
        "                b_input_ids, b_input_mask, b_labels = batch\r\n",
        "\r\n",
        "                with torch.no_grad():\r\n",
        "                    logits = model(b_input_ids, attention_mask=b_input_mask, token_type_ids=None)\r\n",
        "\r\n",
        "                logits = logits.detach().cpu().numpy()\r\n",
        "                label_ids = b_labels.squeeze(1).to('cpu').numpy()\r\n",
        "                batch_preds = np.argmax(logits, axis=1)\r\n",
        "                batch_labels = np.hstack(label_ids)\r\n",
        "\r\n",
        "                valid_preds.extend(batch_preds)\r\n",
        "                valid_labels.extend(batch_labels)\r\n",
        "\r\n",
        "            with open('log'+str(fold)+'.txt', 'a') as f:\r\n",
        "                f.write(\"Accuracy: {0:.2f}%\".format(\r\n",
        "                  accuracy_score(valid_labels, valid_preds) * 100\r\n",
        "                ))\r\n",
        "                f.write(\"Matthews: {0:.2f}%\".format(\r\n",
        "                  matthews_corrcoef(valid_labels, valid_preds) * 100\r\n",
        "                ))\r\n",
        "            model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v87esspjVCb"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYtgZWLjjXGk"
      },
      "source": [
        "device = 'cuda'\n",
        "bert_config = BertConfig.from_json_file('./gdrive/My Drive/Transformer/bert_config.json')\n",
        "model = BertForSequenceClassification(bert_config, 5)\n",
        "model.to(device)\n",
        "model = torch.nn.DataParallel(model)\n",
        "model.load_state_dict(torch.load('./gdrive/My Drive/weights5000.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxws_7e3jh_1"
      },
      "source": [
        "tokenizer = tokenization.FullTokenizer(vocab_file='./gdrive/My Drive/Transformer/vocab.txt', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svG7T-R5jsb7"
      },
      "source": [
        "def predict(sentence, tokenizer, max_len):\n",
        "#     print(sentence)\n",
        "    tokens_a = tokenizer.tokenize(sentence)\n",
        "    if len(tokens_a) > max_len - 2:\n",
        "        tokens_a = tokens_a[0:(max_len - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_mask = [1] * len(input_ids)\n",
        "    while len(input_ids) < max_len:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "    input_ids = torch.LongTensor(input_ids).unsqueeze(0)\n",
        "    input_mask = torch.LongTensor(input_mask).unsqueeze(0)\n",
        "    a = time.time()\n",
        "\n",
        "    logits = model(input_ids.cuda(), None, input_mask.cuda())\n",
        "    logits = logits.squeeze(0)\n",
        "    \n",
        "    b = time.time()\n",
        "    logits = F.softmax(logits, dim=-1)\n",
        "    clas = logits.argmax().item()\n",
        "    return clas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSxzKBPAjuEH"
      },
      "source": [
        "maps[predict(\"Василий Ломаченко боксировал с Теофимо Лопесом с травмой плеча\", tokenizer, 512)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBeGRKDRjyh8"
      },
      "source": [
        "maps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muCrXSoOpD_R"
      },
      "source": [
        "Accuracy = [88.39, 90.11, 89.93, 91.26, 91.32, 90.96, 90.93, 90.75]\r\n",
        "Matthews = [82.75, 85.16, 85.02, 86.91, 86.98, 86.42, 86.42, 86.13]\r\n",
        "Mean_loss = [0.24487, 0.16310, 0.10926, 0.09439, 0.05900, 0.04465, 0.02807,  0.01792]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1PGbpxSpiMY"
      },
      "source": [
        "plt.plot(Accuracy)\r\n",
        "plt.plot(Matthews)\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n68BnGtbpmvr"
      },
      "source": [
        "plt.plot(Mean_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dohI8tkFqElH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}