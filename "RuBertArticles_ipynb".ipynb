{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3lxGLwdM4Bf",
    "outputId": "7c436e0b-462f-4176-af42-83c940a994d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tIC6AQ5hKgtV"
   },
   "outputs": [],
   "source": [
    "!cp \"/Users/iluzaangirova/Desktop/Bert/modeling.py\" .\n",
    "!cp \"/Users/iluzaangirova/Desktop/Bert/optimization.py\" .\n",
    "!cp \"/Users/iluzaangirova/Desktop/Bert/tokenization.py\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V2_VPDaUJLTy"
   },
   "outputs": [],
   "source": [
    "import tokenization \n",
    "import torch\n",
    "import sys \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from modeling import BertConfig, BertForSequenceClassification\n",
    "from optimization import BERTAdam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "Ob5ceiKvIxj4",
    "outputId": "75a4465c-40f2-4d23-8c70-4a9d6a24cc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33068, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport</td>\n",
       "      <td>Суд американского города Торранс приступит к р...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sport</td>\n",
       "      <td>Состояние здоровья знаменитого канадского хокк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>Финский \"Йокерит\" потеряет преимущество своей ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>\"Сакраменто\" проиграл \"Мемфису\" в матче чемпио...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sport</td>\n",
       "      <td>ФК \"Валенсия\": сделаем все, чтобы найти фаната...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topics                                              title\n",
       "0  sport  Суд американского города Торранс приступит к р...\n",
       "1  sport  Состояние здоровья знаменитого канадского хокк...\n",
       "2  sport  Финский \"Йокерит\" потеряет преимущество своей ...\n",
       "3  sport  \"Сакраменто\" проиграл \"Мемфису\" в матче чемпио...\n",
       "4  sport  ФК \"Валенсия\": сделаем все, чтобы найти фаната..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/iluzaangirova/Desktop/Bert/dataset_5.csv', encoding='utf8')\n",
    "df = df[['topics', 'title']]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "TGHclaZXJErP",
    "outputId": "008b561a-7be9-4606-8f19-71e67d55b05d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sport', 'society', 'auto', 'business', 'economic'], dtype='object')\n",
      "   topics                                              title\n",
      "0       0  Суд американского города Торранс приступит к р...\n",
      "1       0  Состояние здоровья знаменитого канадского хокк...\n",
      "2       0  Финский \"Йокерит\" потеряет преимущество своей ...\n",
      "3       0  \"Сакраменто\" проиграл \"Мемфису\" в матче чемпио...\n",
      "4       0  ФК \"Валенсия\": сделаем все, чтобы найти фаната...\n",
      "Max длина предложения title: 162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maps = pd.factorize(df.topics)[1]\n",
    "print(maps)\n",
    "df['topics'] = pd.factorize(df.topics)[0]\n",
    "print(df.head())\n",
    "df['title'] = df['title'].astype('str')\n",
    "for c in df:\n",
    "    if df[c].dtype == 'object':\n",
    "        print('Max длина предложения %s: %s\\n' %  (c, df[c].map(len).max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAlqGGkUj43W"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M0854Ic2K2zm"
   },
   "outputs": [],
   "source": [
    "sentences = df['title'].values\n",
    "labels = df['topics'].to_list()\n",
    "assert len(sentences) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BGrem6pgJlUO"
   },
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "euvah3iaJwgO"
   },
   "outputs": [],
   "source": [
    "train_sentences, test_sentences, train_gt, test_gt = train_test_split(sentences, labels, shuffle=True,test_size=0.3, random_state=42)\n",
    "assert len(set(train_gt)) == len(set(test_gt))\n",
    "num_classes = len(set(train_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "W5vBhys1LIRA"
   },
   "outputs": [],
   "source": [
    "def preprocessing(sentences, labels, tokenizer, max_len):\n",
    "    features = []\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        \n",
    "        tokens_a = tokenizer.tokenize(sentence)\n",
    "        \n",
    "        if len(tokens_a) > max_len - 2:\n",
    "            tokens_a = tokens_a[0:(max_len - 2)]\n",
    "\n",
    "        tokens = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "        tokens.append(\"[SEP]\")\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_len:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            \n",
    "        assert len(input_ids) == max_len\n",
    "        assert len(input_mask) == max_len\n",
    "\n",
    "        features.append(\n",
    "                    InputFeatures(\n",
    "                            input_ids=input_ids,\n",
    "                            input_mask=input_mask,\n",
    "                            label=[labels[i]]))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OEphZufULkz5"
   },
   "outputs": [],
   "source": [
    "class Dataload(torch.utils.data.Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \n",
    "        return torch.LongTensor(self.features[index].input_ids),\\\n",
    "               torch.LongTensor(self.features[index].input_mask),\\\n",
    "               torch.LongTensor(self.features[index].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './gdrive/My Drive/Transformer/vocab.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-18bf38e5060f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./gdrive/My Drive/Transformer/vocab.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/RuBertArticles/tokenization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordpieceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RuBertArticles/tokenization.py\u001b[0m in \u001b[0;36mload_vocab\u001b[0;34m(vocab_file)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './gdrive/My Drive/Transformer/vocab.txt'"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file='./gdrive/My Drive/Transformer/vocab.txt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 22045 TEST: 11023 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5286d6c486df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAIN:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TEST:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(sentences, labels)):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index), fold)\n",
    "    train_sentences, test_sentences = sentences[train_index], sentences[test_index]\n",
    "    train_gt, test_gt = labels[train_index], labels[test_index]\n",
    "    \n",
    "    features = preprocessing(train_sentences, train_gt, tokenizer, 512)\n",
    "    dataset_train = Dataload(features)\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset_train,batch_size = 8, shuffle=True,\\\n",
    "                                                  num_workers=6, pin_memory=True)\n",
    "    features = preprocessing(test_sentences, test_gt, tokenizer, 512)\n",
    "    dataset_test = Dataload(features)\n",
    "    test_dataloader = torch.utils.data.DataLoader(dataset_test,batch_size = 1, shuffle=False,\\\n",
    "                                                  num_workers=6, pin_memory=True)\n",
    "    \n",
    "    print(len(train_dataloader))\n",
    "    print(len(test_dataloader))\n",
    "\n",
    "    device = 'cuda'\n",
    "    bert_config = BertConfig.from_json_file('./gdrive/My Drive/Transformer/bert_config.json')\n",
    "    model = BertForSequenceClassification(bert_config, num_classes)\n",
    "    model.bert.load_state_dict(torch.load('./gdrive/My Drive/Transformer/pytorch_model.bin'\\\n",
    "                                          , map_location='cpu'))\n",
    "    model.to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    num_epoch = 5\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    num_train_steps = len(train_dataloader) * num_epoch\n",
    "    optimizer = BERTAdam(optimizer_parameters,\n",
    "                        lr=5e-5,\n",
    "                        warmup=0.1,\n",
    "                        t_total=num_train_steps)\n",
    "    \n",
    "    f = open('log'+str(fold)+'.txt', 'w')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    train_loss_set = []\n",
    "\n",
    "    batch_iterator = iter(train_dataloader)\n",
    "\n",
    "    total_step = 0\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    while total_step<5*len(train_dataloader):\n",
    "\n",
    "        total_step += 1\n",
    "        try:\n",
    "            batch = next(batch_iterator)\n",
    "        except:\n",
    "            batch_iterator = iter(data_loader)\n",
    "            batch = next(batch_iterator)\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, logits = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels.squeeze(1), token_type_ids=None)\n",
    "        \n",
    "        train_loss_set.append(loss.mean().item())\n",
    "\n",
    "        loss.mean().backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.mean().item()\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.plot(train_loss_set)\n",
    "        plt.title(\"Training loss\")\n",
    "        plt.xlabel(\"Batch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "        if total_step%1000 == 0:\n",
    "            print(\"Mean loss: {0:.5f}\".format(train_loss / len(train_dataloader)))\n",
    "            with open('log'+str(fold)+'.txt', 'a') as f:\n",
    "                f.write(\"Mean loss: {0:.5f}\\n\".format(train_loss / len(train_dataloader)))\n",
    "            torch.save(model.state_dict(), './gdrive/My Drive/weights'+str(fold)+'_'+str(total_step)+'.pth')\n",
    "            train_loss = 0\n",
    "            model.eval()\n",
    "            valid_preds, valid_labels = [], []\n",
    "\n",
    "            for batch in test_dataloader:   \n",
    "\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids, attention_mask=b_input_mask, token_type_ids=None)\n",
    "\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.squeeze(1).to('cpu').numpy()\n",
    "                batch_preds = np.argmax(logits, axis=1)\n",
    "                batch_labels = np.hstack(label_ids)\n",
    "\n",
    "                valid_preds.extend(batch_preds)\n",
    "                valid_labels.extend(batch_labels)\n",
    "\n",
    "            with open('log'+str(fold)+'.txt', 'a') as f:\n",
    "                f.write(\"Accuracy: {0:.2f}%\".format(\n",
    "                  accuracy_score(valid_labels, valid_preds) * 100\n",
    "                ))\n",
    "                f.write(\"Matthews: {0:.2f}%\".format(\n",
    "                  matthews_corrcoef(valid_labels, valid_preds) * 100\n",
    "                ))\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v87esspjVCb"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RYtgZWLjjXGk",
    "outputId": "e4073083-81b5-4238-b0a7-9299b66457ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "bert_config = BertConfig.from_json_file('/Users/iluzaangirova/Desktop/Bert/train_predict/bert_config.json')\n",
    "model = BertForSequenceClassification(bert_config, 5)\n",
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('/Users/iluzaangirova/Desktop/Bert/weights5000.pth'\\\n",
    "                                      , map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wxws_7e3jh_1"
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file='/Users/iluzaangirova/Desktop/Bert/vocab.txt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "svG7T-R5jsb7"
   },
   "outputs": [],
   "source": [
    "def predict(sentence, tokenizer, max_len):\n",
    "#     print(sentence)\n",
    "    tokens_a = tokenizer.tokenize(sentence)\n",
    "    if len(tokens_a) > max_len - 2:\n",
    "        tokens_a = tokens_a[0:(max_len - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    while len(input_ids) < max_len:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "    input_ids = torch.LongTensor(input_ids).unsqueeze(0)\n",
    "    input_mask = torch.LongTensor(input_mask).unsqueeze(0)\n",
    "    a = time.time()\n",
    "\n",
    "    logits = model(input_ids.cpu(), None, input_mask.cpu())\n",
    "    logits = logits.squeeze(0)\n",
    "    \n",
    "    b = time.time()\n",
    "    logits = F.softmax(logits, dim=-1)\n",
    "    clas = logits.argmax().item()\n",
    "    return clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "nSxzKBPAjuEH",
    "outputId": "877505cf-a3e6-49c6-c2db-806195db7a07",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'society'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"В Молдавии приостановили действие поправок о полномочиях президента\"\n",
    "maps[predict(t, tokenizer, 512)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "uBeGRKDRjyh8",
    "outputId": "5858acd7-76aa-4f12-84d7-8bf4d8ae5d25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sport', 'society', 'auto', 'business', 'economic'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "J6a0Sryrj1Wb"
   },
   "outputs": [],
   "source": [
    "window = tk.Tk()\n",
    "window.title('Classifier')\n",
    "label = tk.Label(text=\"Введите заголовок статьи\")\n",
    "entry = tk.Entry()\n",
    "label.pack()\n",
    "entry.pack()\n",
    "title1 = entry.get()\n",
    "def button():\n",
    "    label = tk.Label(window, text=maps[predict(entry.get(), tokenizer, 512)])\n",
    "    label.pack()\n",
    "btn1 = tk.Button(window, text = 'классифицировать', command = button)\n",
    "btn1.pack()\n",
    "window.mainloop()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " \"RuBertArticles.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
